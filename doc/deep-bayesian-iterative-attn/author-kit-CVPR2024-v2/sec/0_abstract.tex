\begin{abstract}
Isotropic neural networks was considered to be worse in performance compared with pyramid-like neural networks, although 
We propose here that isotropic neural networks coupled with weight-sharing iterative class attentions properly could achieve competitive performance with pyramid-like neural networks, both in CNN based and Transformer based architectures.
We find that proper temperature value in the class attention is critical for the performance of isotropic neural networks.
Post-norm.
In addition to their performance, [isotropic neural networks] have the advantage of explainable. we could naturally visualize how the attention evolved layer by layer.

% Look at previous \confName abstracts to get a feel for style and length.
\end{abstract}